{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Toronto AI](https://i.imgur.com/diILtDP.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "A trained neural net can be thought of as a learned mapping.\n",
    "\n",
    "Here are some examples of mappings that a neural net could learn:\n",
    "\n",
    "*   Mapping English to French\n",
    "*   Mapping pictures to text descriptions\n",
    "*   Mapping live sensor data from a reusable rocket to control commands that land it\n",
    "*   Mapping random vectors into images of flowers\n",
    "\n",
    "In essence, we use neural nets to map one distribution of data onto another.\n",
    "\n",
    "Here's an an example where I trained a neural net to map random vectors onto the space of flower photos, using a Generative Adversarial Network:\n",
    "\n",
    "![](https://i.imgur.com/SaT9OEM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors\n",
    "\n",
    "* Tensors are multidimensional arrays.\n",
    "* They are like boxes of data, that we use to contain our data, or the weights of our model.\n",
    "* Tensors are used extensively in TensorFlow to represent:\n",
    "  * 0-D - scalars\n",
    "  * 1-D - vectors, text\n",
    "  * 2-D - matrices, tables of data\n",
    "  * 3-D - batches of matrices, a cube of data, e.g. an image\n",
    "  * 4-D - convolution kernels, a monochrome video\n",
    "  * 5-D - color video\n",
    "  * 6-D - 3D vector fields, batches of colour video\n",
    "  * 7-D - batches of 3D vector fields\n",
    "  * 8-D - batches of layered 3D vector fields \n",
    "  * 9-D - batches of layered 3D vector fields through time\n",
    "  * keep going...\n",
    "\n",
    "* GPU memory is expensive, so Tensors are most commonly 4-D or less.\n",
    "\n",
    "* It helps to visualize a Tensor as a Rubiks Cube - each cell holds a piece of scalar data (like a weight, a piece of input data, or a label).  For higher dimensional Tensors, think of each cell as holding a Tensor instead of a scalar.\n",
    "![](https://i.imgur.com/KyOQVX9.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random, math\n",
    "from matplotlib import animation, rc\n",
    "from IPython.display import HTML\n",
    "\n",
    "plt.rcParams['image.cmap'] = 'viridis'\n",
    "\n",
    "np.random.seed(20180118)\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "DIM = 64\n",
    "TWO_PI = 2.*math.pi\n",
    "MIN_CIRCLE_WIDTH = int(DIM/8)\n",
    "NUM_CIRCLE_DOTS = 500\n",
    "CIRCLE_WIDTH = 3\n",
    "\n",
    "# We'll use this to create our data set.\n",
    "def add_circles(data):\n",
    "    radius = int((random.random() * (DIM/2 - MIN_CIRCLE_WIDTH)) + MIN_CIRCLE_WIDTH)\n",
    "    xpos = random.random()*DIM\n",
    "    ypos = random.random()*DIM\n",
    "    \n",
    "    draw_circle(data, xpos, ypos, radius)\n",
    "\n",
    "    return [xpos, ypos, radius+CIRCLE_WIDTH/2]\n",
    "\n",
    "\n",
    "def draw_circle(data, xpos, ypos, radius):\n",
    "    for i in range(NUM_CIRCLE_DOTS):\n",
    "        for r in range(radius, radius+CIRCLE_WIDTH):\n",
    "            rad = TWO_PI * i/NUM_CIRCLE_DOTS\n",
    "            x = round(r*math.cos(rad)+xpos) \n",
    "            y = round(-r*math.sin(rad)+ypos)\n",
    "            if x >= 0 and x < DIM and y >= 0 and y < DIM:\n",
    "                data[x,y] = data[x,y] / 1.75\n",
    "\n",
    "\n",
    "            \n",
    "# Create random noise and draw circles in it\n",
    "def create_dataset_row():\n",
    "    data = np.random.random((DIM, DIM)).astype(np.float32)\n",
    "    label = add_circles(data)\n",
    "    label = np.array(label).astype(np.float32)\n",
    "    \n",
    "    return (data, label)\n",
    "    \n",
    "\n",
    "def create_dataset(rows):\n",
    "    \n",
    "    labels  = []\n",
    "    samples = []\n",
    "    for i in range(rows):\n",
    "        data, label = create_dataset_row()\n",
    "        labels.append(label)\n",
    "        samples.append(data)\n",
    "    return (np.array(samples).astype(np.float32), np.array(labels).astype(np.float32))\n",
    "\n",
    "        \n",
    "\n",
    "# Create an animation so we can see our data set\n",
    "dataset = create_dataset(BATCH_SIZE)\n",
    "data   = dataset[0]\n",
    "labels = dataset[1]\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "def init():\n",
    "    ax.cla()\n",
    "    return ()\n",
    "\n",
    "def animate(i):\n",
    "    ax.imshow(data[i%len(data)])\n",
    "    return ()\n",
    "\n",
    "anim = animation.FuncAnimation(fig, animate, init_func=init, frames=10, interval=700, blit=True)\n",
    "plt.close(fig)\n",
    "fig.set_size_inches(10, 10, True)\n",
    "HTML(anim.to_jshtml())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge\n",
    "\n",
    "* We're searching for a signal in noisy image data.\n",
    "\n",
    "* In this particular problem, we want to know the location and width of the circle hidden in the image.\n",
    "\n",
    "* It would be complex to write a rule-based program (using lots of if conditions).  A good choice here is to use a neural network.\n",
    "\n",
    "* Since we're using image data, we'll use a convolutional neural network.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/o2qIsu4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Layers\n",
    "\n",
    "\n",
    "* a convolutional layer is used to learn a set of translation-invariant feature detectors, called filters.\n",
    "* they are *much* more lightweight than a fully connected layer\n",
    "* these are particularly useful for image data\n",
    "* also useful in other areas of machine learning.\n",
    "\n",
    "\n",
    "> ### Side quest: Reversible Residual Network\n",
    "> * ResNets use convolutional layers - they can be very deep and very effective, but they use a lot of memory.\n",
    "> * A recent paper from the University of Toronto shows how to reduce the memory footprint by typically at least an order of magnitude.\n",
    "> * The Reversible Residual Network: Backpropagation Without Storing Activations\n",
    "> * https://arxiv.org/abs/1707.04585 \n",
    "> * *Aidan N. Gomez, Mengye Ren, Raquel Urtasun, Roger B. Grosse*\n",
    "\n",
    "\n",
    "### Filters (a.k.a. feature detectors)\n",
    "* each filter has a small window through which it can read the input, and that window is passed across the entire image\n",
    "* the weights are applied to the input channels at each position, and the result is written to the output channel for that filter.\n",
    "* A convolutional layer with many filters will have an output channel for each filter, holding the results.\n",
    "\n",
    "### Activation functions\n",
    "* To add 'depth' to our convolutional layer (i.e. the depth in deep learning), we need to add a non-linearity to the output, called an activation function.\n",
    "* We'll use a Leaky ReLu - it's nonlinear and simple.\n",
    "\n",
    "![Leaky ReLu](https://i.imgur.com/KxYFRIL.png)\n",
    "\n",
    "Further reading on activation functions:\n",
    "* https://arxiv.org/abs/1505.00853 - Empirical Evaluation of Rectified Activations in Convolutional Network\n",
    "* https://arxiv.org/abs/1709.06247 - Training Better CNNs Requires to Rethink ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Let's create a method that will add a convolutional layer to our graph\n",
    "\n",
    "\n",
    "def Convolution(layer, name, num_filters, size=3, activation=tf.nn.relu): \n",
    "\n",
    "    with tf.variable_scope(\"detector\", reuse=tf.AUTO_REUSE):\n",
    "        \n",
    "        layer = tf.layers.conv2d( \n",
    "              name=name, \n",
    "              inputs=layer,\n",
    "              kernel_size=size,\n",
    "              filters=num_filters,\n",
    "              dilation_rate=1,\n",
    "              padding='same',\n",
    "              activation=activation\n",
    "        )\n",
    "        return layer\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def AvgPooling(layer): \n",
    "    return tf.layers.average_pooling2d(layer, pool_size=(2,2), strides=(2,2))\n",
    "\n",
    "def MaxPooling(layer): \n",
    "    return tf.layers.max_pooling2d(layer, pool_size=(2,2), strides=(2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input pipeline\n",
    "\n",
    "* Here we create an input pipeline for our data.\n",
    "* This pipeline serves the purpose of generating training and test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import itertools\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# \n",
    "def row_generator():\n",
    "  for i in itertools.count(1):\n",
    "    yield create_dataset_row()\n",
    "\n",
    "\n",
    "    \n",
    "def create_input_pipeline():\n",
    "\n",
    "    dataset = tf.data.Dataset.from_generator(row_generator, (tf.float32, tf.float32))\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.repeat()\n",
    "        \n",
    "    pipeline_iterator = dataset.make_initializable_iterator()\n",
    "    \n",
    "    return pipeline_iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating our graph\n",
    "\n",
    "## Subgraph: The convolutional layers\n",
    "\n",
    "* We are stacking our convolutional layers, so that later layers detect features on lower layers.\n",
    "* Higher layers learn higher level features from lower layers that learn lower level features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def convolutional_layers(batch):\n",
    "    \n",
    "    layer = tf.reshape(batch, [BATCH_SIZE, DIM, DIM, 1])  # We need a channel dimension, adding one here.\n",
    "    \n",
    "    with tf.variable_scope(\"convolutional\"):\n",
    "        \n",
    "        F = 32\n",
    "        KS = 7\n",
    "        \n",
    "        \n",
    "#         layer = MaxPooling(layer)\n",
    "        layer = Convolution(layer, \"A1\", num_filters=F, size=KS, activation=None)    # 32x32 pixels\n",
    "        layer = Convolution(layer, \"A2\", num_filters=F, size=KS, activation=None)\n",
    "        layer = Convolution(layer, \"A3\", num_filters=F, size=KS)\n",
    "        layer = Convolution(layer, \"A4\", num_filters=F, size=KS)\n",
    "        layer = Convolution(layer, \"A5\", num_filters=F, size=KS)\n",
    "        layer = Convolution(layer, \"A6\", num_filters=F, size=KS, activation=None)\n",
    "        layer = Convolution(layer, \"A7\", num_filters=F, size=KS, activation=None)\n",
    "        layer = Convolution(layer, \"A8\", num_filters=F, size=KS)\n",
    "        \n",
    "        return layer\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subgraph: The fully connected dense layers\n",
    "\n",
    "* These layers are used to convert the tensor that was output from the convolutional layers down into a prediction.\n",
    "* In our case, we want 3 outputs\n",
    "  \n",
    "  * The X and Y coordinate of the center of the circle\n",
    "  * The radius of the circle\n",
    "\n",
    "* Here we're dividing the channels into thirds, and we have attached one subnet of fully connected layers to each output.  \n",
    "* We are asking the neural net that the sum of the outputs of each subnet is the corresponding prediction (for x, y, radius)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def dense_layers(layer): \n",
    "    \n",
    "    layer = tf.reshape(layer, [BATCH_SIZE, -1])    # Make it a table, one row for each example in the batch.\n",
    "    \n",
    "    x_guess = add_subnet(layer, 0)  # Let's create a subnet for predicting the X coordinate,\n",
    "    y_guess = add_subnet(layer, 1)  # A subnet for the y coordinate,\n",
    "    r_guess = add_subnet(layer, 2)  # and, a subnet for predicting the radius\n",
    "    \n",
    "    y_guess2 = add_subnet(layer, 1)  # A subnet for the y coordinate,\n",
    "    \n",
    "    return x_guess, (y_guess+y_guess2)/2., r_guess\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def add_subnet(layer, i):\n",
    "    ROW_DIM = 1\n",
    "    a_third  = int(layer.shape[ROW_DIM].value / 3)\n",
    "    \n",
    "    layer = tf.slice(layer, [0, i*a_third], [BATCH_SIZE, a_third])\n",
    "    \n",
    "    neurons = 16\n",
    "    \n",
    "    layer = tf.layers.dense(layer, neurons, activation=tf.nn.leaky_relu)\n",
    "    layer = tf.layers.dense(layer, neurons, activation=tf.nn.leaky_relu)\n",
    "    layer = tf.layers.dense(layer, neurons, activation=tf.nn.leaky_relu)\n",
    "    layer = tf.layers.dense(layer, neurons, activation=tf.nn.leaky_relu)\n",
    "\n",
    "\n",
    "    # Return the sum of the last layer as our prediction\n",
    "    return tf.reduce_sum(layer, axis=[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective function (a.k.a. the loss function)\n",
    "\n",
    "* The objective function is what the system attempts to minimize\n",
    "* The most important thing to remember is that the loss function needs to be differentiable with a minimum value at your goal.\n",
    "* Convex functions are easy to minimize.\n",
    "\n",
    "### Common objectives\n",
    "* Minimizing the difference of squares (a.k.a. mean squared error)\n",
    "\n",
    "* Minimizing the log loss - this is useful in classification tasks when dealing with probabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loss_function(x, y, r, labels):\n",
    "\n",
    "    \n",
    "    actual_x, actual_y, actual_r = labels[:,0], labels[:,1], labels[:,2]\n",
    "    \n",
    "    \n",
    "    # Compute the mean squared error.\n",
    "    \n",
    "    x_error = tf.pow(x - actual_x, 2.)\n",
    "    y_error = tf.pow(y - actual_y, 2.)\n",
    "    r_error = tf.pow(r - actual_r, 2.)\n",
    "    \n",
    "    loss = x_error + y_error + r_error\n",
    "    \n",
    "    tf.summary.scalar(\"x_error\",      tf.sqrt(tf.reduce_sum(x_error)/BATCH_SIZE))\n",
    "    tf.summary.scalar(\"y_error\",      tf.sqrt(tf.reduce_sum(y_error)/BATCH_SIZE))\n",
    "    tf.summary.scalar(\"radius_error\", tf.sqrt(tf.reduce_sum(r_error)/BATCH_SIZE))\n",
    "    \n",
    "    return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer\n",
    "\n",
    "There are many choices for optimizers.\n",
    "For most applications, the Adam optimizer will give you good flexibility and fast training.\n",
    "\n",
    "## Adam\n",
    "\n",
    "* The adam optimizer is a gradient descent optimization alogirthm that adds two things:\n",
    "  * First, it adds momentum to each weight of your model to help it descend.\n",
    "  * Second, it slows down weights proportionally to how much they are oscillating \n",
    "  \n",
    "* Both of the effects of the Adam optimizer have an exponential decay built in.  These are parameters to the optimizer.\n",
    "  * alpha - The learning rate.  Typical values are 0.0003 to to 0.000003\n",
    "  * beta1 - The decay rate of the momentum term.  Typical values are 0.5 to 0.9.\n",
    "  * beta2 - The decay rate of the variance term.  Typical values are 0.9 to 0.999\n",
    "\n",
    "\n",
    "\n",
    "Further reading: \n",
    "* https://arxiv.org/abs/1412.6980 Adam: A Method for Stochastic Optimization\n",
    "* http://ruder.io/optimizing-gradient-descent/ An overview of gradient descent optimization algorithms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ALPHA = 0.0001\n",
    "BETA1 = 0.5\n",
    "BETA2 = 0.9\n",
    "\n",
    "\n",
    "def add_optimizer(loss):\n",
    "    \n",
    "    opt      = tf.train.AdamOptimizer(learning_rate = ALPHA, beta1=BETA1, beta2=BETA2)\n",
    "    train_op = opt.minimize(loss=loss)\n",
    "    return train_op\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connecting the model pieces together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_model(samples, labels):\n",
    "\n",
    "    x, y, r = dense_layers( convolutional_layers( samples ) )\n",
    "\n",
    "    loss = loss_function(x, y, r, labels)\n",
    "\n",
    "    training_op = add_optimizer(loss)\n",
    "\n",
    "    return {\n",
    "        \"training_op\"   : training_op,\n",
    "        \"loss\"          : tf.reduce_sum(loss),\n",
    "        \"prediction\"    : (x, y, r),\n",
    "        \"actual\"        : labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing a Training run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "def run_session(session, run_name, num_iterations):\n",
    "    \n",
    "    UPDATE_TENSORBOARD_PERIOD = 5\n",
    "\n",
    "    with tf.device(\"/device:CPU:0\"):\n",
    "            #\n",
    "        fw = tf.summary.FileWriter(\"/home/titan/dev/ai/circles/\" + run_name, graph=session.graph, flush_secs=2)\n",
    "\n",
    "    \n",
    "        #\n",
    "        pipeline = create_input_pipeline()\n",
    "\n",
    "        sample_batch, labels_batch = pipeline.get_next()\n",
    "\n",
    "        #\n",
    "        model = create_model(sample_batch, labels_batch)\n",
    "\n",
    "        # \n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "        #\n",
    "        session.run(pipeline.initializer)\n",
    "\n",
    "        #\n",
    "        for step in range(num_iterations):\n",
    "            \n",
    "            new_batch = session.run(sample_batch)\n",
    "\n",
    "            results = session.run({\n",
    "                \"loss\"       : model[\"loss\"],\n",
    "                \"train_op\"   : model[\"training_op\"],\n",
    "                \"prediction\" : model[\"prediction\"],\n",
    "                \"actual\"     : model[\"actual\"],\n",
    "                \"summary\"    : tf.summary.merge_all()\n",
    "            })\n",
    "            \n",
    "            \n",
    "            \n",
    "            if step % UPDATE_TENSORBOARD_PERIOD == 0:\n",
    "                print(\"Total loss: %d.\" % results[\"loss\"])\n",
    "#                 print(results)\n",
    "                fw.add_summary(results[\"summary\"], step)\n",
    "        \n",
    "        return results\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "session = tf.Session()\n",
    "\n",
    "with session:\n",
    "    \n",
    "    results = run_session(session, \"firstRun\", 1)\n",
    "    print(results)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the Results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import itertools\n",
    "\n",
    "def r(x):\n",
    "    return random.random()*x\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    dataset = tf.data.Dataset.from_generator(gen, (tf.float32, tf.float32))\n",
    "#     dataset = tf.data.Dataset.range(1)\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.map(lambda x,y: create_dataset(32))\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "        \n",
    "    pipeline_iterator = dataset.make_initializable_iterator()\n",
    "    \n",
    "    sess.run(pipeline_iterator.initializer)\n",
    "\n",
    "    next_batch = pipeline_iterator.get_next()\n",
    "    results = sess.run(next_batch)\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thank you!\n",
    "\n",
    "That was a lot of info!\n",
    "Thank you for your time and hopefully this will help you on your path.\n",
    "\n",
    "For your troubles, here's a flower:\n",
    "![](https://i.imgur.com/i6wAoY9.png)\n",
    "<p style=\"text-align:center\">AI generated flower, orchestration by Dave MacDonald</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect with the Toronto AI community\n",
    "\n",
    "<img src=\"https://cdn.worldvectorlogo.com/logos/slack-1.svg\" style=\"display:inline;width: 30px;padding-right:1em;\"/> Come join us on our Slack Channel!  (Here is the <a href=\"https://join.slack.com/t/toronto-ai/shared_invite/enQtMjE5NTM5MzY3NTU0LTQ0ZDIyM2ZlZDYwMmRjY2I2NTEyMjZjYzJkNzljZTI1ZWRiMDkzYjUyZjRkMTc5ZDM0OGJmZjdmNzM5NDM5Zjk\">Invite link</a> if you haven't signed up yet)\n",
    "\n",
    "<img src=\"https://cdn.worldvectorlogo.com/logos/meetup.svg\" style=\"display:inline;width: 30px;padding-right:1em;\"/> Come to our next event - join our [Meetup Group](https://www.meetup.com/Toronto-AI/)</img>\n",
    "\n",
    "<img src=\"https://i.imgur.com/KGSBbUe.png\" style=\"display:inline;width:30px;padding-right:1em;3\"/> NEO tips <span style='font-size:.75em'>_AdLG9AyRtCMSeAy98rmkkos7uFU6i7fLgd_</span>\n",
    "\n",
    "\n",
    "Toronto AI website: [http://torontoai.org/](http://torontoai.org/)<br>\n",
    "We're also on Facebook and Twitter\n",
    "<br><br>\n",
    "Thank you, see you soon!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "* [Deep Learning Book](http://www.deeplearningbook.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
